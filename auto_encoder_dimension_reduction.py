# -*- coding: utf-8 -*-
"""Auto_Encoder_Dimension_Reduction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dWi54zEAIX8osRAp2TKWp_caWjmy-Jxc
"""

import numpy as np
import os
import random
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.model_selection import GroupShuffleSplit

# NOTE:
# Latent features are extracted using an encoder trained
# exclusively on the training split to prevent representation leakage.

# -----------------------------
# Reproducibility
# -----------------------------
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# -----------------------------
# Autoencoder architecture
# -----------------------------
def build_autoencoder(input_dim):
    input_layer = layers.Input(shape=(input_dim,))
    x = layers.Dense(256, activation='relu')(input_layer)
    latent = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(256, activation='relu')(latent)
    output_layer = layers.Dense(input_dim, activation='linear')(x)

    autoencoder = models.Model(input_layer, output_layer)
    encoder = models.Model(input_layer, latent)

    optimizer = optimizers.Adam(learning_rate=1e-3)
    autoencoder.compile(optimizer=optimizer, loss='mse')
    return autoencoder, encoder

# Load subject-level features (user-provided)
df = pd.read_csv("data/EfficientNetB7_features.csv")

groups = df["subject_id"].values
X = df.drop(columns=["label", "subject_id"]).values
y = df["label"].values

"""**Subject-wise split used as a precautionary measure to prevent any potential data leakage; all samples from the same subject are kept in a single split.**"""

# -----------------------------
# NO LEAKAGE
# -----------------------------
# X: extracted deep features
# y: labels (pMCI / sMCI)
# subject-wise split must be applied before this step


gss = GroupShuffleSplit(n_splits=1, test_size=0.4, random_state=SEED)
train_idx, test_idx = next(gss.split(X, y, groups))

X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]


# Standardization (fit on TRAIN only)
scaler = StandardScaler().fit(X_train)
X_train_std = scaler.transform(X_train)
X_test_std  = scaler.transform(X_test)

# Train autoencoder ONLY on training data
autoencoder, encoder = build_autoencoder(X_train_std.shape[1])
autoencoder.fit(
    X_train_std, X_train_std,
    epochs=30,
    batch_size=32,
    shuffle=True,
    verbose=0
)

# Latent features
Z_train = encoder.predict(X_train_std)
Z_test  = encoder.predict(X_test_std)

"""**Note: Each row in X corresponds to a unique subject. Subject-level
partitioning was performed prior to slice extraction.**
"""

os.makedirs("outputs", exist_ok=True)

# -----------------------------
# Save TRAIN features
# -----------------------------
df_train = pd.DataFrame(Z_train)
df_train["label"] = y_train
df_train.to_csv("outputs/AE_train_features.csv", index=False)
print("AE_train_features.csv file created")

# -----------------------------
# Save TEST features
# -----------------------------
df_test = pd.DataFrame(Z_test)
df_test["label"] = y_test
df_test.to_csv("outputs/AE_test_features.csv", index=False)
print("AE_test_features.csv file created")

